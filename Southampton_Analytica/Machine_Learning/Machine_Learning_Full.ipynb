{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Load the Pandas libraries with alias 'pd' \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import max_norm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn import model_selection\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Read data\n",
    "#EI\n",
    "train_data_EI = pd.read_csv(\"../data/traintest/mbti_EI_train\")\n",
    "train_data_EI_target = pd.read_csv(\"../data/traintest/mbti_EI_train_target\", header=None, usecols=[1])\n",
    "test_data_EI = pd.read_csv(\"../data/traintest/mbti_EI_test\")\n",
    "test_data_EI_target = pd.read_csv(\"../data/traintest/mbti_EI_test_target\", header=None, usecols=[1])\n",
    "#FT\n",
    "train_data_FT = pd.read_csv(\"../data/traintest/mbti_FT_train\")\n",
    "train_data_FT_target = pd.read_csv(\"../data/traintest/mbti_FT_train_target\", header=None, usecols=[1])\n",
    "test_data_FT = pd.read_csv(\"../data/traintest/mbti_FT_test\")\n",
    "test_data_FT_target = pd.read_csv(\"../data/traintest/mbti_FT_test_target\", header=None, usecols=[1])\n",
    "#NS\n",
    "train_data_NS = pd.read_csv(\"../data/traintest/mbti_NS_train\")\n",
    "train_data_NS_target = pd.read_csv(\"../data/traintest/mbti_NS_train_target\", header=None, usecols=[1])\n",
    "test_data_NS = pd.read_csv(\"../data/traintest/mbti_NS_test\")\n",
    "test_data_NS_target = pd.read_csv(\"../data/traintest/mbti_NS_test_target\", header=None, usecols=[1])\n",
    "#PJ\n",
    "train_data_PJ = pd.read_csv(\"../data/traintest/mbti_PJ_train\")\n",
    "train_data_PJ_target = pd.read_csv(\"../data/traintest/mbti_PJ_train_target\", header=None, usecols=[1])\n",
    "test_data_PJ = pd.read_csv(\"../data/traintest/mbti_PJ_test\")\n",
    "test_data_PJ_target = pd.read_csv(\"../data/traintest/mbti_PJ_test_target\", header=None, usecols=[1])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1], dtype='int64')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_data_EI.head() \n",
    "train_data_EI_target.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0  1\n",
       "1  0\n",
       "2  1\n",
       "3  1\n",
       "4  0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#covert targets to binary \n",
    "\n",
    "#training data manipulation\n",
    "train_data_EI_target[1] = train_data_EI_target[1].map({'I': '1', 'E': '0'}) #change Intravert'I'/Extrovert'E' to 1 and 0 for logistical regression\n",
    "train_data_FT_target[1] = train_data_FT_target[1].map({'T': '1', 'F': '0'}) #change T/F to 1 and 0 for logistical regression\n",
    "train_data_NS_target[1] = train_data_NS_target[1].map({'N': '1', 'S': '0'}) #change N/S to 1 and 0 for logistical regression\n",
    "train_data_PJ_target[1] = train_data_PJ_target[1].map({'P': '1', 'J': '0'}) #change P/J to 1 and 0 for logistical regression\n",
    "train_data_EI_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1\n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  1\n",
       "4  1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test data manipulation\n",
    "test_data_EI_target[1] = test_data_EI_target[1].map({'I': '1', 'E': '0'}) #change Intravert'I'/Extrovert'E' to 1 and 0 for logistical regression\n",
    "test_data_FT_target[1] = test_data_FT_target[1].map({'T': '1', 'F': '0'}) #change T/F to 1 and 0 for logistical regression\n",
    "test_data_NS_target[1] = test_data_NS_target[1].map({'N': '1', 'S': '0'}) #change N/S to 1 and 0 for logistical regression\n",
    "test_data_PJ_target[1] = test_data_PJ_target[1].map({'P': '1', 'J': '0'}) #change P/J to 1 and 0 for logistical regression\n",
    "test_data_FT_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:9: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if __name__ == '__main__':\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:13: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  del sys.path[0]\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:16: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  app.launch_new_instance()\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:17: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "#scale data\n",
    "#EI\n",
    "train_EI_scaler = preprocessing.StandardScaler().fit(train_data_EI)\n",
    "train_EI_scaled = train_EI_scaler.transform(train_data_EI)\n",
    "test_EI_scaled = train_EI_scaler.transform(test_data_EI)\n",
    "#FT\n",
    "train_FT_scaler = preprocessing.StandardScaler().fit(train_data_FT)\n",
    "train_FT_scaled = train_EI_scaler.transform(train_data_FT)\n",
    "test_FT_scaled = train_EI_scaler.transform(test_data_FT)\n",
    "#NS\n",
    "train_NS_scaler = preprocessing.StandardScaler().fit(train_data_NS)\n",
    "train_NS_scaled = train_EI_scaler.transform(train_data_NS)\n",
    "test_NS_scaled = train_EI_scaler.transform(test_data_NS)\n",
    "#PJ\n",
    "train_PJ_scaler = preprocessing.StandardScaler().fit(train_data_PJ)\n",
    "train_PJ_scaled = train_EI_scaler.transform(train_data_PJ)\n",
    "test_PJ_scaled = train_EI_scaler.transform(test_data_PJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score EI:\n",
      "0.82625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_EI.sav']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logostic regression\n",
    "\n",
    "#-----EI-----\n",
    "\n",
    "logreg=LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "logreg.fit(train_EI_scaled,train_data_EI_target.values.ravel())\n",
    "train_pred_EI = cross_validate(logreg, train_EI_scaled, train_data_EI_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_EI = logreg.predict(test_EI_scaled)\n",
    "accuracy_EI = accuracy_score(test_data_EI_target, test_pred_EI)\n",
    "print(\"Test Accuracy Score EI:\")\n",
    "print(accuracy_EI)\n",
    "\n",
    "filename = 'logreg_EI.sav'\n",
    "joblib.dump(logreg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score FT:\n",
      "0.9516635279347144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_FT.sav']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "#----FT----\n",
    "\n",
    "logreg=LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "logreg.fit(train_FT_scaled,train_data_FT_target.values.ravel())\n",
    "train_pred_FT = cross_validate(logreg, train_FT_scaled, train_data_FT_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_FT = logreg.predict(test_FT_scaled)\n",
    "accuracy_FT = accuracy_score(test_data_FT_target, test_pred_FT)\n",
    "print(\"Test Accuracy Score FT:\")\n",
    "print(accuracy_FT)\n",
    "\n",
    "filename = 'logreg_FT.sav'\n",
    "joblib.dump(logreg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score NS:\n",
      "0.7432150313152401\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_NS.sav']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "#----NS----\n",
    "\n",
    "logreg=LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "logreg.fit(train_NS_scaled,train_data_NS_target.values.ravel())\n",
    "train_pred_NS = cross_validate(logreg, train_NS_scaled, train_data_NS_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_NS = logreg.predict(test_NS_scaled)\n",
    "accuracy_NS = accuracy_score(test_data_NS_target, test_pred_NS)\n",
    "print(\"Test Accuracy Score NS:\")\n",
    "print(accuracy_NS)\n",
    "\n",
    "filename = 'logreg_NS.sav'\n",
    "joblib.dump(logreg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score PJ:\n",
      "0.9090247452692868\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['logreg_PJ.sav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#logistic regression\n",
    "\n",
    "#----PJ----\n",
    "\n",
    "logreg=LogisticRegression(solver='lbfgs', max_iter=500)\n",
    "logreg.fit(train_PJ_scaled,train_data_PJ_target.values.ravel())\n",
    "train_pred_PJ = cross_validate(logreg, train_PJ_scaled, train_data_PJ_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_PJ = logreg.predict(test_PJ_scaled)\n",
    "accuracy_PJ = accuracy_score(test_data_PJ_target, test_pred_PJ)\n",
    "print(\"Test Accuracy Score PJ:\")\n",
    "print(accuracy_PJ)\n",
    "\n",
    "filename = 'logreg_PJ.sav'\n",
    "joblib.dump(logreg, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Claudia\\TensorFlow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Score EI:\n",
      "0.825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['randomforest_EI.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#random forest\n",
    "\n",
    "rft1 = RandomForestClassifier()\n",
    "rft1.fit(train_EI_scaled,train_data_EI_target.values.ravel())\n",
    "train_pred_EI = cross_validate(rft1, train_EI_scaled, train_data_EI_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_EI = rft1.predict(test_EI_scaled)\n",
    "accuracy_EI = accuracy_score(test_data_EI_target, test_pred_EI)\n",
    "print(\"Test Accuracy Score EI:\")\n",
    "print(accuracy_EI)\n",
    "\n",
    "filename = 'randomforest_EI.sav'\n",
    "joblib.dump(rft1, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Claudia\\TensorFlow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Test Accuracy Score FT:\n",
      "0.8832391713747646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['randomforest_FT.sav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft1 = RandomForestClassifier()\n",
    "rft1.fit(train_FT_scaled,train_data_FT_target.values.ravel())\n",
    "train_pred_FT = cross_validate(rft1, train_FT_scaled, train_data_FT_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_FT = rft1.predict(test_FT_scaled)\n",
    "accuracy_FT = accuracy_score(test_data_FT_target, test_pred_FT)\n",
    "print(\"Random forest Test Accuracy Score FT:\")\n",
    "print(accuracy_FT)\n",
    "\n",
    "filename = 'randomforest_FT.sav'\n",
    "joblib.dump(rft1, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Claudia\\TensorFlow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Test Accuracy Score NS:\n",
      "0.7807933194154488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['randomforest_NS.sav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft1 = RandomForestClassifier()\n",
    "rft1.fit(train_NS_scaled,train_data_NS_target.values.ravel())\n",
    "train_pred_NS = cross_validate(rft1, train_NS_scaled, train_data_NS_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_NS = rft1.predict(test_NS_scaled)\n",
    "accuracy_NS = accuracy_score(test_data_NS_target, test_pred_NS)\n",
    "print(\"Random forest Test Accuracy Score NS:\")\n",
    "print(accuracy_NS)\n",
    "\n",
    "filename = 'randomforest_NS.sav'\n",
    "joblib.dump(rft1, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Claudia\\TensorFlow\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest Test Accuracy Score PJ:\n",
      "0.7933042212518195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['randomforest_PJ.sav']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rft1 = RandomForestClassifier()\n",
    "rft1.fit(train_PJ_scaled,train_data_PJ_target.values.ravel())\n",
    "train_pred_PJ = cross_validate(rft1, train_PJ_scaled, train_data_PJ_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_PJ = rft1.predict(test_PJ_scaled)\n",
    "accuracy_PJ = accuracy_score(test_data_PJ_target, test_pred_PJ)\n",
    "print(\"Random forest Test Accuracy Score PJ:\")\n",
    "print(accuracy_PJ)\n",
    "\n",
    "filename = 'randomforest_PJ.sav'\n",
    "joblib.dump(rft1, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN\n",
    "batches = 400\n",
    "validation = 0.2\n",
    "epochs = 250\n",
    "neurons = 75\n",
    "l_rate = 0.00005\n",
    "\n",
    "def build_model(neurons, input_dim, epochs, momentum, l2reg):    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_dim = input_dim, kernel_constraint=max_norm(3),\n",
    "                    kernel_regularizer=regularizers.l2(l2reg),\n",
    "                    activation = 'relu'))\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(neurons, input_dim = input_dim, kernel_constraint=max_norm(3),\n",
    "                    kernel_regularizer=regularizers.l2(l2reg),\n",
    "                    activation = 'relu'))    \n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(neurons, input_dim = input_dim, kernel_constraint=max_norm(3),\n",
    "                    kernel_regularizer=regularizers.l2(l2reg),\n",
    "                    activation = 'relu'))\n",
    "    model.add(Dropout(0.6))   \n",
    "    model.add(Dense(neurons, input_dim = input_dim, kernel_constraint=max_norm(3),\n",
    "                    kernel_regularizer=regularizers.l2(l2reg),\n",
    "                    activation = 'relu'))\n",
    "  #  model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer= 'adam', metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2558 samples, validate on 640 samples\n",
      "Epoch 1/250\n",
      "2558/2558 [==============================] - 2s 868us/step - loss: 0.7509 - acc: 0.5102 - val_loss: 0.7290 - val_acc: 0.5469\n",
      "Epoch 2/250\n",
      "2558/2558 [==============================] - 1s 332us/step - loss: 0.7346 - acc: 0.5375 - val_loss: 0.7264 - val_acc: 0.5531\n",
      "Epoch 3/250\n",
      "2558/2558 [==============================] - 1s 335us/step - loss: 0.7285 - acc: 0.5450 - val_loss: 0.7189 - val_acc: 0.6094\n",
      "Epoch 4/250\n",
      "2558/2558 [==============================] - 1s 326us/step - loss: 0.7036 - acc: 0.5942 - val_loss: 0.6957 - val_acc: 0.6781\n",
      "Epoch 5/250\n",
      "2558/2558 [==============================] - 1s 331us/step - loss: 0.6469 - acc: 0.6603 - val_loss: 0.6434 - val_acc: 0.7000\n",
      "Epoch 6/250\n",
      "2558/2558 [==============================] - 1s 324us/step - loss: 0.5403 - acc: 0.7592 - val_loss: 0.6078 - val_acc: 0.7188\n",
      "Epoch 7/250\n",
      "2558/2558 [==============================] - 1s 326us/step - loss: 0.4541 - acc: 0.8307 - val_loss: 0.5928 - val_acc: 0.7344\n",
      "Epoch 8/250\n",
      "2558/2558 [==============================] - 1s 323us/step - loss: 0.3348 - acc: 0.8933 - val_loss: 0.6514 - val_acc: 0.7438\n",
      "Epoch 9/250\n",
      "2558/2558 [==============================] - 1s 326us/step - loss: 0.2601 - acc: 0.9222 - val_loss: 0.7466 - val_acc: 0.7578\n",
      "Epoch 10/250\n",
      "2558/2558 [==============================] - 1s 325us/step - loss: 0.2152 - acc: 0.9406 - val_loss: 0.8122 - val_acc: 0.7438\n",
      "3198/3198 [==============================] - 0s 128us/step\n",
      "800/800 [==============================] - 0s 122us/step\n",
      "training results  [0.21835831584894635, 0.9474671669420859]\n",
      "testing results  [0.6929662978649139, 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NN_EI.sav']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EI\n",
    "model_EI = build_model(neurons,train_EI_scaled.shape[1],  epochs, 0.85, 0.0001)\n",
    "callbacks1 = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint(filepath='best_model_t1.h5', monitor='val_loss', save_best_only=True)]\n",
    "model_EI_trained = model_EI.fit(x = train_EI_scaled, y = train_data_EI_target, callbacks=callbacks1,epochs = epochs, batch_size = 64, verbose = True,  validation_split= validation)\n",
    "eval_train = model_EI.evaluate(x = train_EI_scaled, y =train_data_EI_target)\n",
    "eval_test = model_EI.evaluate(x = test_EI_scaled, y = test_data_EI_target)\n",
    "print('training results ', eval_train )\n",
    "print('testing results ', eval_test )\n",
    "\n",
    "filename = 'NN_EI.sav'\n",
    "joblib.dump(model_EI, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5095 samples, validate on 1274 samples\n",
      "Epoch 1/250\n",
      "5095/5095 [==============================] - 2s 451us/step - loss: 0.7452 - acc: 0.5111 - val_loss: 0.7214 - val_acc: 0.6193\n",
      "Epoch 2/250\n",
      "5095/5095 [==============================] - 2s 349us/step - loss: 0.7088 - acc: 0.5819 - val_loss: 0.6404 - val_acc: 0.7771\n",
      "Epoch 3/250\n",
      "5095/5095 [==============================] - 2s 351us/step - loss: 0.5718 - acc: 0.7441 - val_loss: 0.4459 - val_acc: 0.8422\n",
      "Epoch 4/250\n",
      "5095/5095 [==============================] - 2s 332us/step - loss: 0.4075 - acc: 0.8553 - val_loss: 0.3897 - val_acc: 0.8658\n",
      "Epoch 5/250\n",
      "5095/5095 [==============================] - 2s 334us/step - loss: 0.2968 - acc: 0.9056 - val_loss: 0.3807 - val_acc: 0.8689\n",
      "Epoch 6/250\n",
      "5095/5095 [==============================] - 2s 332us/step - loss: 0.2188 - acc: 0.9394 - val_loss: 0.4122 - val_acc: 0.8736\n",
      "Epoch 7/250\n",
      "5095/5095 [==============================] - 2s 334us/step - loss: 0.1840 - acc: 0.9529 - val_loss: 0.4218 - val_acc: 0.8744\n",
      "Epoch 8/250\n",
      "5095/5095 [==============================] - 2s 355us/step - loss: 0.1510 - acc: 0.9653 - val_loss: 0.4983 - val_acc: 0.8666\n",
      "6369/6369 [==============================] - 1s 164us/step\n",
      "1593/1593 [==============================] - 0s 156us/step\n",
      "FT training results  [0.15155760070970284, 0.9725231590516564]\n",
      "FT testing results  [0.4483552561080164, 0.8731952290525993]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NN_FT.sav']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FT\n",
    "model_FT = build_model(neurons,train_FT_scaled.shape[1],  epochs, 0.85, 0.0001)\n",
    "callbacks1 = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint(filepath='best_model_t1.h5', monitor='val_loss', save_best_only=True)]\n",
    "model_FT_trained = model_FT.fit(x = train_FT_scaled, y = train_data_FT_target, callbacks=callbacks1,epochs = epochs, batch_size = 64, verbose = True,  validation_split= validation)\n",
    "eval_train = model_FT.evaluate(x = train_FT_scaled, y =train_data_FT_target)\n",
    "eval_test = model_FT.evaluate(x = test_FT_scaled, y = test_data_FT_target)\n",
    "print('FT training results ', eval_train )\n",
    "print('FT testing results ', eval_test )\n",
    "\n",
    "filename = 'NN_FT.sav'\n",
    "joblib.dump(model_FT, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1532 samples, validate on 383 samples\n",
      "Epoch 1/250\n",
      "1532/1532 [==============================] - 1s 781us/step - loss: 0.7486 - acc: 0.5163 - val_loss: 0.7342 - val_acc: 0.4883\n",
      "Epoch 2/250\n",
      "1532/1532 [==============================] - 0s 325us/step - loss: 0.7446 - acc: 0.5261 - val_loss: 0.7334 - val_acc: 0.4935\n",
      "Epoch 3/250\n",
      "1532/1532 [==============================] - 1s 327us/step - loss: 0.7458 - acc: 0.5131 - val_loss: 0.7304 - val_acc: 0.5535\n",
      "Epoch 4/250\n",
      "1532/1532 [==============================] - 0s 313us/step - loss: 0.7226 - acc: 0.5627 - val_loss: 0.7285 - val_acc: 0.5535\n",
      "Epoch 5/250\n",
      "1532/1532 [==============================] - 1s 341us/step - loss: 0.7167 - acc: 0.5646 - val_loss: 0.7227 - val_acc: 0.5953\n",
      "Epoch 6/250\n",
      "1532/1532 [==============================] - 1s 330us/step - loss: 0.7031 - acc: 0.5888 - val_loss: 0.7134 - val_acc: 0.6162\n",
      "Epoch 7/250\n",
      "1532/1532 [==============================] - 1s 327us/step - loss: 0.6542 - acc: 0.6495 - val_loss: 0.7000 - val_acc: 0.6266\n",
      "Epoch 8/250\n",
      "1532/1532 [==============================] - 1s 332us/step - loss: 0.5962 - acc: 0.7076 - val_loss: 0.6814 - val_acc: 0.6449\n",
      "Epoch 9/250\n",
      "1532/1532 [==============================] - 1s 333us/step - loss: 0.5137 - acc: 0.7820 - val_loss: 0.6670 - val_acc: 0.6580\n",
      "Epoch 10/250\n",
      "1532/1532 [==============================] - 1s 327us/step - loss: 0.4354 - acc: 0.8251 - val_loss: 0.6775 - val_acc: 0.6606\n",
      "Epoch 11/250\n",
      "1532/1532 [==============================] - 1s 329us/step - loss: 0.3417 - acc: 0.8727 - val_loss: 0.6994 - val_acc: 0.6762\n",
      "Epoch 12/250\n",
      "1532/1532 [==============================] - 1s 328us/step - loss: 0.2918 - acc: 0.9021 - val_loss: 0.7302 - val_acc: 0.6919\n",
      "1915/1915 [==============================] - 0s 136us/step\n",
      "479/479 [==============================] - 0s 139us/step\n",
      "NS training results  [0.23208987109032375, 0.9321148821641508]\n",
      "NS testing results  [0.7231961085552463, 0.6805845516459678]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NN_NS.sav']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NS\n",
    "model_NS = build_model(neurons,train_NS_scaled.shape[1],  epochs, 0.85, 0.0001)\n",
    "callbacks1 = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint(filepath='best_model_t1.h5', monitor='val_loss', save_best_only=True)]\n",
    "model_NS_trained = model_NS.fit(x = train_NS_scaled, y = train_data_NS_target, callbacks=callbacks1,epochs = epochs, batch_size = 64, verbose = True,  validation_split= validation)\n",
    "eval_train = model_NS.evaluate(x = train_NS_scaled, y =train_data_NS_target)\n",
    "eval_test = model_NS.evaluate(x = test_NS_scaled, y = test_data_NS_target)\n",
    "print('NS training results ', eval_train )\n",
    "print('NS testing results ', eval_test )\n",
    "\n",
    "filename = 'NN_NS.sav'\n",
    "joblib.dump(model_NS, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4395 samples, validate on 1099 samples\n",
      "Epoch 1/250\n",
      "4395/4395 [==============================] - 2s 500us/step - loss: 0.7534 - acc: 0.4933 - val_loss: 0.7318 - val_acc: 0.5250\n",
      "Epoch 2/250\n",
      "4395/4395 [==============================] - 1s 329us/step - loss: 0.7420 - acc: 0.5135 - val_loss: 0.7308 - val_acc: 0.5532\n",
      "Epoch 3/250\n",
      "4395/4395 [==============================] - 1s 330us/step - loss: 0.7324 - acc: 0.5395 - val_loss: 0.7260 - val_acc: 0.6187\n",
      "Epoch 4/250\n",
      "4395/4395 [==============================] - 1s 329us/step - loss: 0.7103 - acc: 0.5804 - val_loss: 0.6936 - val_acc: 0.6870\n",
      "Epoch 5/250\n",
      "4395/4395 [==============================] - 1s 329us/step - loss: 0.6507 - acc: 0.6694 - val_loss: 0.6021 - val_acc: 0.7607\n",
      "Epoch 6/250\n",
      "4395/4395 [==============================] - 1s 336us/step - loss: 0.5101 - acc: 0.7784 - val_loss: 0.5118 - val_acc: 0.7989\n",
      "Epoch 7/250\n",
      "4395/4395 [==============================] - 1s 329us/step - loss: 0.3708 - acc: 0.8733 - val_loss: 0.4874 - val_acc: 0.8098\n",
      "Epoch 8/250\n",
      "4395/4395 [==============================] - 1s 329us/step - loss: 0.2706 - acc: 0.9172 - val_loss: 0.5014 - val_acc: 0.8080\n",
      "Epoch 9/250\n",
      "4395/4395 [==============================] - 1s 329us/step - loss: 0.2397 - acc: 0.9358 - val_loss: 0.5045 - val_acc: 0.8153\n",
      "Epoch 10/250\n",
      "4395/4395 [==============================] - 1s 327us/step - loss: 0.2038 - acc: 0.9520 - val_loss: 0.5212 - val_acc: 0.8180\n",
      "5494/5494 [==============================] - 1s 136us/step\n",
      "1374/1374 [==============================] - 0s 128us/step\n",
      "PJ training results  [0.16702921324571263, 0.9628685840182102]\n",
      "PJ testing results  [0.4795658898145351, 0.8275109166835246]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NN_PJ.sav']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PJ\n",
    "model_PJ = build_model(neurons,train_PJ_scaled.shape[1],  epochs, 0.85, 0.0001)\n",
    "callbacks1 = [EarlyStopping(monitor='val_loss', patience=3),\n",
    "             ModelCheckpoint(filepath='best_model_t1.h5', monitor='val_loss', save_best_only=True)]\n",
    "model_PJ_trained = model_PJ.fit(x = train_PJ_scaled, y = train_data_PJ_target, callbacks=callbacks1,epochs = epochs, batch_size = 64, verbose = True,  validation_split= validation)\n",
    "eval_train = model_PJ.evaluate(x = train_PJ_scaled, y =train_data_PJ_target)\n",
    "eval_test = model_PJ.evaluate(x = test_PJ_scaled, y = test_data_PJ_target)\n",
    "print('PJ training results ', eval_train )\n",
    "print('PJ testing results ', eval_test )\n",
    "\n",
    "filename = 'NN_PJ.sav'\n",
    "joblib.dump(model_PJ, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "#EI\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(train_EI_scaled, train_data_EI_target.values.ravel())\n",
    "train_pred_EI = cross_validate(clf, train_EI_scaled, train_data_EI_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_EI = clf.predict(test_EI_scaled)\n",
    "accuracy_EI = accuracy_score(test_data_EI_target, test_pred_EI)\n",
    "print(\"Test Accuracy Score EI:\")\n",
    "print(accuracy_EI)\n",
    "\n",
    "filename = 'svm_EI.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FT\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(train_FT_scaled, train_data_FT_target.values.ravel())\n",
    "train_pred_FT = cross_validate(clf, train_FT_scaled, train_data_FT_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_FT = clf.predict(test_FT_scaled)\n",
    "accuracy_FT = accuracy_score(test_data_FT_target, test_pred_FT)\n",
    "print(\"Test Accuracy Score FT:\")\n",
    "print(accuracy_FT)\n",
    "\n",
    "filename = 'svm_FT.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NS\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(train_NS_scaled, train_data_NS_target.values.ravel())\n",
    "train_pred_NS = cross_validate(clf, train_NS_scaled, train_data_NS_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_NS = clf.predict(test_NS_scaled)\n",
    "accuracy_NS = accuracy_score(test_data_NS_target, test_pred_NS)\n",
    "print(\"Test Accuracy Score NS:\")\n",
    "print(accuracy_NS)\n",
    "\n",
    "filename = 'svm_NS.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PJ\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(train_PJ_scaled, train_data_PJ_target.values.ravel())\n",
    "train_pred_PJ = cross_validate(clf, train_PJ_scaled, train_data_PJ_target.values.ravel(), cv=10, scoring ='accuracy')\n",
    "test_pred_PJ = clf.predict(test_PJ_scaled)\n",
    "accuracy_PJ = accuracy_score(test_data_PJ_target, test_pred_PJ)\n",
    "print(\"Test Accuracy Score PJ:\")\n",
    "print(accuracy_PJ)\n",
    "\n",
    "filename = 'svm_PJ.sav'\n",
    "joblib.dump(clf, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import model_selection\n",
    "#from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49749843652282677\n",
      "0.503061705134244\n",
      "0.5018276762402089\n",
      "0.49963596650891884\n"
     ]
    }
   ],
   "source": [
    "print(sum(np.asfarray(train_data_EI_target[1],float)) / len(train_data_EI_target[1]))\n",
    "print(sum(np.asfarray(train_data_FT_target[1],float)) / len(train_data_FT_target[1]))\n",
    "print(sum(np.asfarray(train_data_NS_target[1],float)) / len(train_data_NS_target[1]))\n",
    "print(sum(np.asfarray(train_data_PJ_target[1],float)) / len(train_data_PJ_target[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "predict_data = pd.read_csv(\"../data/yelp_pre-processed_2.csv\")\n",
    "predict_data_r  = predict_data.drop([predict_data.columns[1],'user_id','text','text.1','posts','joined_comment'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:5: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predict_data_scaled_EI = train_EI_scaler.transform(predict_data_r)\n",
    "predict_data_scaled_NS = train_NS_scaler.transform(predict_data_r)\n",
    "predict_data_scaled_FT = train_FT_scaler.transform(predict_data_r)\n",
    "predict_data_scaled_PJ = train_PJ_scaler.transform(predict_data_r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65109, 5023)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#EI\n",
    "filename = 'logreg_EI.sav'\n",
    "loaded_model_EI = joblib.load(filename)\n",
    "result_EI = loaded_model_EI.predict(predict_data_scaled_EI)\n",
    "#FT\n",
    "filename = 'logreg_FT.sav'\n",
    "loaded_model_FT = joblib.load(filename)\n",
    "result_FT = loaded_model_FT.predict(predict_data_scaled_FT)\n",
    "#NS\n",
    "filename = 'randomforest_NS.sav'\n",
    "loaded_model_NS = joblib.load(filename)\n",
    "result_NS = loaded_model_NS.predict(predict_data_scaled_NS)\n",
    "#PJ\n",
    "filename = 'logreg_PJ.sav'\n",
    "loaded_model_PJ = joblib.load(filename)\n",
    "result_PJ = loaded_model_PJ.predict(predict_data_scaled_PJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0' '0' '0' ... '1' '1' '1']\n",
      "['1' '1' '0' ... '1' '1' '1']\n",
      "['1' '0' '0' ... '0' '0' '0']\n",
      "['1' '1' '1' ... '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(result_EI)\n",
    "print(result_FT)\n",
    "print(result_NS)\n",
    "print(result_PJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.954875670030257\n",
      "0.8953293707475157\n",
      "0.13468184122010782\n",
      "0.08665468675605523\n"
     ]
    }
   ],
   "source": [
    "print(sum(np.asfarray(result_EI,float)) / len(result_EI))\n",
    "print(sum(np.asfarray(result_FT,float)) / len(result_FT))\n",
    "print(sum(np.asfarray(result_NS,float)) / len(result_NS))\n",
    "print(sum(np.asfarray(result_PJ,float)) / len(result_PJ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>posts</th>\n",
       "      <th>joined_comment</th>\n",
       "      <th>max_polarity</th>\n",
       "      <th>average_polarity</th>\n",
       "      <th>min_polarity</th>\n",
       "      <th>max_subjectivity</th>\n",
       "      <th>...</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>youyou</th>\n",
       "      <th>yr</th>\n",
       "      <th>yup</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>---1lKK3aKOuomHnwAkAow</td>\n",
       "      <td>I wish I had known more about this when I was ...</td>\n",
       "      <td>['wish', 'known', 'pregnant', 'seems', 'make',...</td>\n",
       "      <td>wish known pregnant seems make sense interesti...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>--2vR0DIsmQ6WfcSzKWigw</td>\n",
       "      <td>Wer mehrere Tage in Las Vegas ist, sollte s ni...</td>\n",
       "      <td>['wer', 'mehrere', 'tage', 'la', 'vega', 'ist'...</td>\n",
       "      <td>wer mehrere tage la vega ist sollte nicht vers...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>--3l8wysfp49Z2TLnyT0vg</td>\n",
       "      <td>This is the best Thai restaurant in Vegas, for...</td>\n",
       "      <td>['best', 'thai', 'restaurant', 'vega', 'matter...</td>\n",
       "      <td>best thai restaurant vega matter best thai res...</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>--3loIEaXcepp9OKGi_1FQ</td>\n",
       "      <td>I got the pasta carbonara and WOW!!! Amazing a...</td>\n",
       "      <td>['got', 'pasta', 'carbonara', 'wow', 'amazing'...</td>\n",
       "      <td>got pasta carbonara wow amazing amazing amazin...</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>--8g9UaBe0xQ4FD0q34h_A</td>\n",
       "      <td>I was really expecting more... I had high hope...</td>\n",
       "      <td>['really', 'expecting', 'high', 'hope', 'went'...</td>\n",
       "      <td>really expecting high hope went dessert ordere...</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 5023 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1                 user_id  \\\n",
       "0           0             0  ---1lKK3aKOuomHnwAkAow   \n",
       "1           1             1  --2vR0DIsmQ6WfcSzKWigw   \n",
       "2           2             2  --3l8wysfp49Z2TLnyT0vg   \n",
       "3           3             3  --3loIEaXcepp9OKGi_1FQ   \n",
       "4           4             4  --8g9UaBe0xQ4FD0q34h_A   \n",
       "\n",
       "                                                text  \\\n",
       "0  I wish I had known more about this when I was ...   \n",
       "1  Wer mehrere Tage in Las Vegas ist, sollte s ni...   \n",
       "2  This is the best Thai restaurant in Vegas, for...   \n",
       "3  I got the pasta carbonara and WOW!!! Amazing a...   \n",
       "4  I was really expecting more... I had high hope...   \n",
       "\n",
       "                                               posts  \\\n",
       "0  ['wish', 'known', 'pregnant', 'seems', 'make',...   \n",
       "1  ['wer', 'mehrere', 'tage', 'la', 'vega', 'ist'...   \n",
       "2  ['best', 'thai', 'restaurant', 'vega', 'matter...   \n",
       "3  ['got', 'pasta', 'carbonara', 'wow', 'amazing'...   \n",
       "4  ['really', 'expecting', 'high', 'hope', 'went'...   \n",
       "\n",
       "                                      joined_comment  max_polarity  \\\n",
       "0  wish known pregnant seems make sense interesti...         1.000   \n",
       "1  wer mehrere tage la vega ist sollte nicht vers...         0.600   \n",
       "2  best thai restaurant vega matter best thai res...         0.613   \n",
       "3  got pasta carbonara wow amazing amazing amazin...         0.650   \n",
       "4  really expecting high hope went dessert ordere...         0.700   \n",
       "\n",
       "   average_polarity  min_polarity  max_subjectivity  ...   youtube  youve  \\\n",
       "0             0.148         -1.00             1.000  ...         0      0   \n",
       "1             0.013         -0.70             1.000  ...         0      0   \n",
       "2             0.112         -0.25             0.667  ...         0      0   \n",
       "3             0.427          0.20             1.000  ...         0      0   \n",
       "4             0.179          0.00             1.000  ...         0      0   \n",
       "\n",
       "   youyou  yr  yup  zen  zero  zodiac  zombie  zone  \n",
       "0       0   0    0    0     1       0       1     0  \n",
       "1       0   0    0    0     0       0       0     0  \n",
       "2       0   0    0    0     0       0       0     0  \n",
       "3       0   0    0    0     0       0       0     0  \n",
       "4       0   0    0    0     0       0       0     0  \n",
       "\n",
       "[5 rows x 5023 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data_n = predict_data\n",
    "predict_data_n['EI'] = result_EI\n",
    "predict_data_n['FT'] = result_FT\n",
    "predict_data_n['NS'] = result_NS\n",
    "predict_data_n['PJ'] = result_PJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65109, 5017)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data_r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>user_id</th>\n",
       "      <th>text</th>\n",
       "      <th>posts</th>\n",
       "      <th>joined_comment</th>\n",
       "      <th>max_polarity</th>\n",
       "      <th>average_polarity</th>\n",
       "      <th>min_polarity</th>\n",
       "      <th>max_subjectivity</th>\n",
       "      <th>...</th>\n",
       "      <th>yup</th>\n",
       "      <th>zen</th>\n",
       "      <th>zero</th>\n",
       "      <th>zodiac</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>EI</th>\n",
       "      <th>FT</th>\n",
       "      <th>NS</th>\n",
       "      <th>PJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>---1lKK3aKOuomHnwAkAow</td>\n",
       "      <td>I wish I had known more about this when I was ...</td>\n",
       "      <td>['wish', 'known', 'pregnant', 'seems', 'make',...</td>\n",
       "      <td>wish known pregnant seems make sense interesti...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.148</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>--2vR0DIsmQ6WfcSzKWigw</td>\n",
       "      <td>Wer mehrere Tage in Las Vegas ist, sollte s ni...</td>\n",
       "      <td>['wer', 'mehrere', 'tage', 'la', 'vega', 'ist'...</td>\n",
       "      <td>wer mehrere tage la vega ist sollte nicht vers...</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>--3l8wysfp49Z2TLnyT0vg</td>\n",
       "      <td>This is the best Thai restaurant in Vegas, for...</td>\n",
       "      <td>['best', 'thai', 'restaurant', 'vega', 'matter...</td>\n",
       "      <td>best thai restaurant vega matter best thai res...</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>--3loIEaXcepp9OKGi_1FQ</td>\n",
       "      <td>I got the pasta carbonara and WOW!!! Amazing a...</td>\n",
       "      <td>['got', 'pasta', 'carbonara', 'wow', 'amazing'...</td>\n",
       "      <td>got pasta carbonara wow amazing amazing amazin...</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>--8g9UaBe0xQ4FD0q34h_A</td>\n",
       "      <td>I was really expecting more... I had high hope...</td>\n",
       "      <td>['really', 'expecting', 'high', 'hope', 'went'...</td>\n",
       "      <td>really expecting high hope went dessert ordere...</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>--BumyUHiO_7YsHurb9Hkw</td>\n",
       "      <td>Food was good but service was lacking. \\r\\r\\n\\...</td>\n",
       "      <td>['food', 'good', 'service', 'lacking', 'loved'...</td>\n",
       "      <td>food good service lacking loved artichoke neve...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.297</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>--HCoE1ghaAlcaAfshICgw</td>\n",
       "      <td>Very attentive staff. Even though it was busy ...</td>\n",
       "      <td>['attentive', 'staff', 'even', 'though', 'busy...</td>\n",
       "      <td>attentive staff even though busy thursday afte...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.258</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>--IYQykIJoVtoae5WChZiw</td>\n",
       "      <td>It's such a great restaurant. We were able to ...</td>\n",
       "      <td>['great', 'restaurant', 'able', 'sit', 'outsid...</td>\n",
       "      <td>great restaurant able sit outside beautiful fa...</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.189</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>0.839</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>--Qh8yKWAvIP4V4K8ZPfHA</td>\n",
       "      <td>I saw The Killers perform here on Sep 22, 2012...</td>\n",
       "      <td>['saw', 'killer', 'perform', 'sep', 'first', '...</td>\n",
       "      <td>saw killer perform sep first time venue expect...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.139</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>--RlSfc-QmcHFGHyX6aVjA</td>\n",
       "      <td>HATE  Nothing.\\r\\r\\n\\r\\r\\nDISLIKE  Location is...</td>\n",
       "      <td>['hate', 'nothing', 'dislike', 'location', 'ki...</td>\n",
       "      <td>hate nothing dislike location kind far home ca...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>--YhjyV-ce1nFLYxP49C5A</td>\n",
       "      <td>I saw this place popped up a few weeks ago and...</td>\n",
       "      <td>['saw', 'place', 'popped', 'week', 'ago', 'int...</td>\n",
       "      <td>saw place popped week ago intrigued looked lik...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.136</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>--ZNfWKj1VyVElRx6-g1fg</td>\n",
       "      <td>So-so banh mi sandwiches.\\r\\r\\n\\r\\r\\nI always ...</td>\n",
       "      <td>['soso', 'banh', 'mi', 'sandwich', 'always', '...</td>\n",
       "      <td>soso banh mi sandwich always get original aka ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.179</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>--ermiD_aia8aiptGGd7CQ</td>\n",
       "      <td>If you were a fan of Liberace you will like th...</td>\n",
       "      <td>['fan', 'liberace', 'like', 'place', 'filled',...</td>\n",
       "      <td>fan liberace like place filled many interestin...</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.159</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>--iPM6WEjeaExHqNHwq8dQ</td>\n",
       "      <td>Nacho daddy is literally my favorite place to ...</td>\n",
       "      <td>['nacho', 'daddy', 'literally', 'favorite', 'p...</td>\n",
       "      <td>nacho daddy literally favorite place eat taco ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>--pBhah9QPQYu7jQft_3Yg</td>\n",
       "      <td>Food was very good. Much more traditional than...</td>\n",
       "      <td>['food', 'good', 'much', 'traditional', 'typic...</td>\n",
       "      <td>food good much traditional typical chinese tak...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.268</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>--yMp9CT8N9uKHw4G7-Xew</td>\n",
       "      <td>The best AYCE sushi restaurant in town. The fo...</td>\n",
       "      <td>['best', 'ayce', 'sushi', 'restaurant', 'town'...</td>\n",
       "      <td>best ayce sushi restaurant town food orgasmic ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.409</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>-00Pu-tHjxzBAyAJgMfiRA</td>\n",
       "      <td>A go-to on every LV trip. Simply the best I've...</td>\n",
       "      <td>['goto', 'every', 'lv', 'trip', 'simply', 'bes...</td>\n",
       "      <td>goto every lv trip simply best ive ever outsid...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>-05XqtNjcBq19vh2CVJN8g</td>\n",
       "      <td>I really enjoy coming to get gelato here. My f...</td>\n",
       "      <td>['really', 'enjoy', 'coming', 'get', 'gelato',...</td>\n",
       "      <td>really enjoy coming get gelato favorite part p...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.313</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>-0CYm85fllm43U7UQOe82w</td>\n",
       "      <td>Ate the breakfast buffet. It was typical buffe...</td>\n",
       "      <td>['ate', 'breakfast', 'buffet', 'typical', 'buf...</td>\n",
       "      <td>ate breakfast buffet typical buffet food wasnt...</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.325</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.917</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>-0HhZbPBlB1YZx3BhAfaEA</td>\n",
       "      <td>Love love love!!! The food is amazing. We are ...</td>\n",
       "      <td>['love', 'love', 'love', 'food', 'amazing', 'k...</td>\n",
       "      <td>love love love food amazing kind regular place...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>-0OyT3le0GqdyvRLLfB7MQ</td>\n",
       "      <td>Amazing ambiance soon as we walked in . We sat...</td>\n",
       "      <td>['amazing', 'ambiance', 'soon', 'walked', 'sat...</td>\n",
       "      <td>amazing ambiance soon walked sat le minuet gre...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>-0PAosnInLQumYYGf-3uFg</td>\n",
       "      <td>This place is a joke. I would give zero stars ...</td>\n",
       "      <td>['place', 'joke', 'would', 'give', 'zero', 'st...</td>\n",
       "      <td>place joke would give zero star part went enti...</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.800</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>-0Pf4ZNOYMm94qfoEFYhaw</td>\n",
       "      <td>Enchilada nachos-delish! Great food, awesome s...</td>\n",
       "      <td>['enchilada', 'nachosdelish', 'great', 'food',...</td>\n",
       "      <td>enchilada nachosdelish great food awesome serv...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.875</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-0ajln3oFI-EViIHzB4eLg</td>\n",
       "      <td>Best cannolis ever. Must get them. They are so...</td>\n",
       "      <td>['best', 'cannolis', 'ever', 'must', 'get', 'a...</td>\n",
       "      <td>best cannolis ever must get awesome love creme...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.289</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>-0b-HvSet-Pg3YPeGpQOKw</td>\n",
       "      <td>I stayed at the hard rock for 4 days,and loved...</td>\n",
       "      <td>['stayed', 'hard', 'rock', 'daysand', 'loved',...</td>\n",
       "      <td>stayed hard rock daysand loved iti say expensi...</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "      <td>-0cuh9IYVLJEVv0e0NGnyA</td>\n",
       "      <td>Always great! Fast and great every time I go. ...</td>\n",
       "      <td>['always', 'great', 'fast', 'great', 'every', ...</td>\n",
       "      <td>always great fast great every time go even dis...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.138</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>-0kOvwKyRPhMtkj41zMLNw</td>\n",
       "      <td>First time in Toronto and stumbled upon this p...</td>\n",
       "      <td>['first', 'time', 'toronto', 'stumbled', 'upon...</td>\n",
       "      <td>first time toronto stumbled upon place yelp fo...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.950</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>-0lFv3aQ5XI9VdiDwsL2vA</td>\n",
       "      <td>We were so excited for this place, but left fe...</td>\n",
       "      <td>['excited', 'place', 'left', 'feeling', 'meh',...</td>\n",
       "      <td>excited place left feeling meh primarily due s...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.216</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>-0mLXBzM3o3giyJb6H4xYw</td>\n",
       "      <td>Had the rib special which really was. It speci...</td>\n",
       "      <td>['rib', 'special', 'really', 'special', 'price...</td>\n",
       "      <td>rib special really special price difference th...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.201</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.900</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>-0tLDaIzN1Gbxc51Bxad4A</td>\n",
       "      <td>I came in here wanting a Vietnamese coffee but...</td>\n",
       "      <td>['came', 'wanting', 'vietnamese', 'coffee', 'd...</td>\n",
       "      <td>came wanting vietnamese coffee dont make told ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>-3PTUP443q6hQESLKSu95w</td>\n",
       "      <td>- What I have tried:\\r\\r\\n   + Cabbage roll (5...</td>\n",
       "      <td>['tried', 'cabbage', 'roll', 'basically', 'mea...</td>\n",
       "      <td>tried cabbage roll basically meat wrapped cabb...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.185</td>\n",
       "      <td>-0.70</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>-3Tgw9br9O68emgrgFj48Q</td>\n",
       "      <td>Food is good. For me most dishes can feed 3 or...</td>\n",
       "      <td>['food', 'good', 'dish', 'feed', 'people', 'di...</td>\n",
       "      <td>food good dish feed people dim sum great appet...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.206</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>72</td>\n",
       "      <td>-3bsS2i9xqjNnIA1fRnzIQ</td>\n",
       "      <td>This place was awesome our rooms where clean a...</td>\n",
       "      <td>['place', 'awesome', 'room', 'clean', 'spaciou...</td>\n",
       "      <td>place awesome room clean spacious lucky enough...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.131</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>-3dLmRlT8nGUdEiDq0KRKw</td>\n",
       "      <td>Kung Fu tea is my absolute favorite boba tea p...</td>\n",
       "      <td>['kung', 'fu', 'tea', 'absolute', 'favorite', ...</td>\n",
       "      <td>kung fu tea absolute favorite boba tea place l...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>-3gUn9YImya8KvHKeHMmFQ</td>\n",
       "      <td>Not bad here. A lot of the same repetitive thi...</td>\n",
       "      <td>['bad', 'lot', 'repetitive', 'thing', 'expecti...</td>\n",
       "      <td>bad lot repetitive thing expecting like one la...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>-3i9bhfvrM3F1wsC9XIB8g</td>\n",
       "      <td>We sat in the very front row, which is not the...</td>\n",
       "      <td>['sat', 'front', 'row', 'ideal', 'perspective'...</td>\n",
       "      <td>sat front row ideal perspective still beautifu...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.156</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>76</td>\n",
       "      <td>-3jtBudQjQpLzlG-apGc5A</td>\n",
       "      <td>Seriously, I had to wait too long for my food....</td>\n",
       "      <td>['seriously', 'wait', 'long', 'food', 'starvin...</td>\n",
       "      <td>seriously wait long food starving come ordered...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.125</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>-3mAqeRyjjb3JJmbYG0vkA</td>\n",
       "      <td>My husband has tried every hotdog on this menu...</td>\n",
       "      <td>['husband', 'tried', 'every', 'hotdog', 'menu'...</td>\n",
       "      <td>husband tried every hotdog menu cant pick didn...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>78</td>\n",
       "      <td>-3paZwdNXO4zRa55ZQtNkQ</td>\n",
       "      <td>Amazing War Museum whats free to get in, But y...</td>\n",
       "      <td>['amazing', 'war', 'museum', 'whats', 'free', ...</td>\n",
       "      <td>amazing war museum whats free get donate like ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.950</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>79</td>\n",
       "      <td>-3s52C4zL_DHRK0ULG6qtg</td>\n",
       "      <td>I really liked this place. The Club Lounge is ...</td>\n",
       "      <td>['really', 'liked', 'place', 'club', 'lounge',...</td>\n",
       "      <td>really liked place club lounge unreal set top ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.147</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>-3vRkE60NTGxdHYlOq4cZA</td>\n",
       "      <td>Great place to relax enjoy gamble and much mor...</td>\n",
       "      <td>['great', 'place', 'relax', 'enjoy', 'gamble',...</td>\n",
       "      <td>great place relax enjoy gamble much staying wy...</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.750</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>81</td>\n",
       "      <td>-4BEUkLvHQntN6qPfKJP2w</td>\n",
       "      <td>This show was freaking amazing.\\r\\r\\n\\r\\r\\nWhe...</td>\n",
       "      <td>['show', 'freaking', 'amazing', 'first', 'walk...</td>\n",
       "      <td>show freaking amazing first walk instantly int...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>82</td>\n",
       "      <td>-4FUt4XC1L98t6mcZf5z1Q</td>\n",
       "      <td>This place was amazing. We just had our weddin...</td>\n",
       "      <td>['place', 'amazing', 'wedding', 'february', 's...</td>\n",
       "      <td>place amazing wedding february service sale st...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.174</td>\n",
       "      <td>-0.94</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>-4G38D90ZA4cmU95nRIjLQ</td>\n",
       "      <td>Best vegan restaurant I have ever been to so f...</td>\n",
       "      <td>['best', 'vegan', 'restaurant', 'ever', 'far',...</td>\n",
       "      <td>best vegan restaurant ever far first place go ...</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.650</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>-4JDJeFS0YAYSiSvIshGLQ</td>\n",
       "      <td>Tried to go there for lunch today. No longer h...</td>\n",
       "      <td>['tried', 'go', 'lunch', 'today', 'longer', 'h...</td>\n",
       "      <td>tried go lunch today longer herelocation vacan...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>-4KIUdEnJ5-V6tloF_hK8g</td>\n",
       "      <td>Check this place out about 2 weeks ago when ta...</td>\n",
       "      <td>['check', 'place', 'week', 'ago', 'taking', 'f...</td>\n",
       "      <td>check place week ago taking family trip la veg...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.161</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>86</td>\n",
       "      <td>-4P27kxN2B6cF5HxSHZUkQ</td>\n",
       "      <td>One of my fave spots downtown! Food and atmosp...</td>\n",
       "      <td>['one', 'fave', 'spot', 'downtown', 'food', 'a...</td>\n",
       "      <td>one fave spot downtown food atmosphere always ...</td>\n",
       "      <td>0.655</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.825</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>87</td>\n",
       "      <td>-4QuO6NErAE2TDvbithCoA</td>\n",
       "      <td>Full disclosure I'm not the target audience pr...</td>\n",
       "      <td>['full', 'disclosure', 'im', 'target', 'audien...</td>\n",
       "      <td>full disclosure im target audience probally ho...</td>\n",
       "      <td>0.356</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.833</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>88</td>\n",
       "      <td>-4cGISljPoTmkKnms2l8AA</td>\n",
       "      <td>I definitely recommend this place if you like ...</td>\n",
       "      <td>['definitely', 'recommend', 'place', 'like', '...</td>\n",
       "      <td>definitely recommend place like thai food atmo...</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.021</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>-4pGdGL_M0zGctI5jeqjAQ</td>\n",
       "      <td>This comedy club is a great place to have a ca...</td>\n",
       "      <td>['comedy', 'club', 'great', 'place', 'casual',...</td>\n",
       "      <td>comedy club great place casual friend night ta...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>-4rdXTtxWLH4qrwIDL_y0A</td>\n",
       "      <td>The food was amazingly good. The meat was tast...</td>\n",
       "      <td>['food', 'amazingly', 'good', 'meat', 'tasty',...</td>\n",
       "      <td>food amazingly good meat tasty tortilla delici...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>91</td>\n",
       "      <td>-4u6fATFg1B0kUzSBki4Nw</td>\n",
       "      <td>When in Vegas, you MUST at least order a Fat T...</td>\n",
       "      <td>['vega', 'must', 'least', 'order', 'fat', 'tue...</td>\n",
       "      <td>vega must least order fat tuesday cannot think...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.307</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>-4viTt9UC44lWCFJwleMNQ</td>\n",
       "      <td>Spectacular, Spectacular, Spectacular, Wonderf...</td>\n",
       "      <td>['spectacular', 'spectacular', 'spectacular', ...</td>\n",
       "      <td>spectacular spectacular spectacular wonderful ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.925</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>-4x8LzD-n-a-AEhAj1TdJw</td>\n",
       "      <td>I have been going to Ted Wiens on eastern ave....</td>\n",
       "      <td>['going', 'ted', 'wiens', 'eastern', 'ave', 'p...</td>\n",
       "      <td>going ted wiens eastern ave past always speak ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.221</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>94</td>\n",
       "      <td>-50XWnmQGqBgEI-9ANvLlg</td>\n",
       "      <td>Experience: Brought the family here while they...</td>\n",
       "      <td>['experience', 'brought', 'family', 'town', 'g...</td>\n",
       "      <td>experience brought family town grandpa use eat...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.205</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>95</td>\n",
       "      <td>-53tUBvXm4opRFYKL-Uv2g</td>\n",
       "      <td>If you're thinking of staying here because of ...</td>\n",
       "      <td>['youre', 'thinking', 'staying', 'convenience'...</td>\n",
       "      <td>youre thinking staying convenience many restau...</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.041</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>-56Ww393sz37tSJ6pUxAbg</td>\n",
       "      <td>Great hotel, great service!  Very classy place...</td>\n",
       "      <td>['great', 'hotel', 'great', 'service', 'classy...</td>\n",
       "      <td>great hotel great service classy place emphasi...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>97</td>\n",
       "      <td>-58OybwYqPGe3mW7FiewNQ</td>\n",
       "      <td>This was our first visit here and we absolutel...</td>\n",
       "      <td>['first', 'visit', 'absolutely', 'loved', 'pla...</td>\n",
       "      <td>first visit absolutely loved place staff nice ...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.360</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>98</td>\n",
       "      <td>-58xgulrXMhEHcUrKvlIuQ</td>\n",
       "      <td>The Golden Knights nachos were amazing and a h...</td>\n",
       "      <td>['golden', 'knight', 'nacho', 'amazing', 'huge...</td>\n",
       "      <td>golden knight nacho amazing huge portion husba...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.199</td>\n",
       "      <td>-0.98</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>-594af_E7Z9VVjQc9pJK3g</td>\n",
       "      <td>WOW! European Wax Center is very consistent wi...</td>\n",
       "      <td>['wow', 'european', 'wax', 'center', 'consiste...</td>\n",
       "      <td>wow european wax center consistent excellent w...</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.195</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 5027 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Unnamed: 0.1                 user_id  \\\n",
       "0            0             0  ---1lKK3aKOuomHnwAkAow   \n",
       "1            1             1  --2vR0DIsmQ6WfcSzKWigw   \n",
       "2            2             2  --3l8wysfp49Z2TLnyT0vg   \n",
       "3            3             3  --3loIEaXcepp9OKGi_1FQ   \n",
       "4            4             4  --8g9UaBe0xQ4FD0q34h_A   \n",
       "5            5             5  --BumyUHiO_7YsHurb9Hkw   \n",
       "6            6             6  --HCoE1ghaAlcaAfshICgw   \n",
       "7            7             7  --IYQykIJoVtoae5WChZiw   \n",
       "8            8             8  --Qh8yKWAvIP4V4K8ZPfHA   \n",
       "9            9             9  --RlSfc-QmcHFGHyX6aVjA   \n",
       "10          10            10  --YhjyV-ce1nFLYxP49C5A   \n",
       "11          11            11  --ZNfWKj1VyVElRx6-g1fg   \n",
       "12          12            12  --ermiD_aia8aiptGGd7CQ   \n",
       "13          13            13  --iPM6WEjeaExHqNHwq8dQ   \n",
       "14          14            14  --pBhah9QPQYu7jQft_3Yg   \n",
       "15          15            15  --yMp9CT8N9uKHw4G7-Xew   \n",
       "16          16            16  -00Pu-tHjxzBAyAJgMfiRA   \n",
       "17          17            17  -05XqtNjcBq19vh2CVJN8g   \n",
       "18          18            18  -0CYm85fllm43U7UQOe82w   \n",
       "19          19            19  -0HhZbPBlB1YZx3BhAfaEA   \n",
       "20          20            20  -0OyT3le0GqdyvRLLfB7MQ   \n",
       "21          21            21  -0PAosnInLQumYYGf-3uFg   \n",
       "22          22            22  -0Pf4ZNOYMm94qfoEFYhaw   \n",
       "23          23            23  -0ajln3oFI-EViIHzB4eLg   \n",
       "24          24            24  -0b-HvSet-Pg3YPeGpQOKw   \n",
       "25          25            25  -0cuh9IYVLJEVv0e0NGnyA   \n",
       "26          26            26  -0kOvwKyRPhMtkj41zMLNw   \n",
       "27          27            27  -0lFv3aQ5XI9VdiDwsL2vA   \n",
       "28          28            28  -0mLXBzM3o3giyJb6H4xYw   \n",
       "29          29            29  -0tLDaIzN1Gbxc51Bxad4A   \n",
       "..         ...           ...                     ...   \n",
       "70          70            70  -3PTUP443q6hQESLKSu95w   \n",
       "71          71            71  -3Tgw9br9O68emgrgFj48Q   \n",
       "72          72            72  -3bsS2i9xqjNnIA1fRnzIQ   \n",
       "73          73            73  -3dLmRlT8nGUdEiDq0KRKw   \n",
       "74          74            74  -3gUn9YImya8KvHKeHMmFQ   \n",
       "75          75            75  -3i9bhfvrM3F1wsC9XIB8g   \n",
       "76          76            76  -3jtBudQjQpLzlG-apGc5A   \n",
       "77          77            77  -3mAqeRyjjb3JJmbYG0vkA   \n",
       "78          78            78  -3paZwdNXO4zRa55ZQtNkQ   \n",
       "79          79            79  -3s52C4zL_DHRK0ULG6qtg   \n",
       "80          80            80  -3vRkE60NTGxdHYlOq4cZA   \n",
       "81          81            81  -4BEUkLvHQntN6qPfKJP2w   \n",
       "82          82            82  -4FUt4XC1L98t6mcZf5z1Q   \n",
       "83          83            83  -4G38D90ZA4cmU95nRIjLQ   \n",
       "84          84            84  -4JDJeFS0YAYSiSvIshGLQ   \n",
       "85          85            85  -4KIUdEnJ5-V6tloF_hK8g   \n",
       "86          86            86  -4P27kxN2B6cF5HxSHZUkQ   \n",
       "87          87            87  -4QuO6NErAE2TDvbithCoA   \n",
       "88          88            88  -4cGISljPoTmkKnms2l8AA   \n",
       "89          89            89  -4pGdGL_M0zGctI5jeqjAQ   \n",
       "90          90            90  -4rdXTtxWLH4qrwIDL_y0A   \n",
       "91          91            91  -4u6fATFg1B0kUzSBki4Nw   \n",
       "92          92            92  -4viTt9UC44lWCFJwleMNQ   \n",
       "93          93            93  -4x8LzD-n-a-AEhAj1TdJw   \n",
       "94          94            94  -50XWnmQGqBgEI-9ANvLlg   \n",
       "95          95            95  -53tUBvXm4opRFYKL-Uv2g   \n",
       "96          96            96  -56Ww393sz37tSJ6pUxAbg   \n",
       "97          97            97  -58OybwYqPGe3mW7FiewNQ   \n",
       "98          98            98  -58xgulrXMhEHcUrKvlIuQ   \n",
       "99          99            99  -594af_E7Z9VVjQc9pJK3g   \n",
       "\n",
       "                                                 text  \\\n",
       "0   I wish I had known more about this when I was ...   \n",
       "1   Wer mehrere Tage in Las Vegas ist, sollte s ni...   \n",
       "2   This is the best Thai restaurant in Vegas, for...   \n",
       "3   I got the pasta carbonara and WOW!!! Amazing a...   \n",
       "4   I was really expecting more... I had high hope...   \n",
       "5   Food was good but service was lacking. \\r\\r\\n\\...   \n",
       "6   Very attentive staff. Even though it was busy ...   \n",
       "7   It's such a great restaurant. We were able to ...   \n",
       "8   I saw The Killers perform here on Sep 22, 2012...   \n",
       "9   HATE  Nothing.\\r\\r\\n\\r\\r\\nDISLIKE  Location is...   \n",
       "10  I saw this place popped up a few weeks ago and...   \n",
       "11  So-so banh mi sandwiches.\\r\\r\\n\\r\\r\\nI always ...   \n",
       "12  If you were a fan of Liberace you will like th...   \n",
       "13  Nacho daddy is literally my favorite place to ...   \n",
       "14  Food was very good. Much more traditional than...   \n",
       "15  The best AYCE sushi restaurant in town. The fo...   \n",
       "16  A go-to on every LV trip. Simply the best I've...   \n",
       "17  I really enjoy coming to get gelato here. My f...   \n",
       "18  Ate the breakfast buffet. It was typical buffe...   \n",
       "19  Love love love!!! The food is amazing. We are ...   \n",
       "20  Amazing ambiance soon as we walked in . We sat...   \n",
       "21  This place is a joke. I would give zero stars ...   \n",
       "22  Enchilada nachos-delish! Great food, awesome s...   \n",
       "23  Best cannolis ever. Must get them. They are so...   \n",
       "24  I stayed at the hard rock for 4 days,and loved...   \n",
       "25  Always great! Fast and great every time I go. ...   \n",
       "26  First time in Toronto and stumbled upon this p...   \n",
       "27  We were so excited for this place, but left fe...   \n",
       "28  Had the rib special which really was. It speci...   \n",
       "29  I came in here wanting a Vietnamese coffee but...   \n",
       "..                                                ...   \n",
       "70  - What I have tried:\\r\\r\\n   + Cabbage roll (5...   \n",
       "71  Food is good. For me most dishes can feed 3 or...   \n",
       "72  This place was awesome our rooms where clean a...   \n",
       "73  Kung Fu tea is my absolute favorite boba tea p...   \n",
       "74  Not bad here. A lot of the same repetitive thi...   \n",
       "75  We sat in the very front row, which is not the...   \n",
       "76  Seriously, I had to wait too long for my food....   \n",
       "77  My husband has tried every hotdog on this menu...   \n",
       "78  Amazing War Museum whats free to get in, But y...   \n",
       "79  I really liked this place. The Club Lounge is ...   \n",
       "80  Great place to relax enjoy gamble and much mor...   \n",
       "81  This show was freaking amazing.\\r\\r\\n\\r\\r\\nWhe...   \n",
       "82  This place was amazing. We just had our weddin...   \n",
       "83  Best vegan restaurant I have ever been to so f...   \n",
       "84  Tried to go there for lunch today. No longer h...   \n",
       "85  Check this place out about 2 weeks ago when ta...   \n",
       "86  One of my fave spots downtown! Food and atmosp...   \n",
       "87  Full disclosure I'm not the target audience pr...   \n",
       "88  I definitely recommend this place if you like ...   \n",
       "89  This comedy club is a great place to have a ca...   \n",
       "90  The food was amazingly good. The meat was tast...   \n",
       "91  When in Vegas, you MUST at least order a Fat T...   \n",
       "92  Spectacular, Spectacular, Spectacular, Wonderf...   \n",
       "93  I have been going to Ted Wiens on eastern ave....   \n",
       "94  Experience: Brought the family here while they...   \n",
       "95  If you're thinking of staying here because of ...   \n",
       "96  Great hotel, great service!  Very classy place...   \n",
       "97  This was our first visit here and we absolutel...   \n",
       "98  The Golden Knights nachos were amazing and a h...   \n",
       "99  WOW! European Wax Center is very consistent wi...   \n",
       "\n",
       "                                                posts  \\\n",
       "0   ['wish', 'known', 'pregnant', 'seems', 'make',...   \n",
       "1   ['wer', 'mehrere', 'tage', 'la', 'vega', 'ist'...   \n",
       "2   ['best', 'thai', 'restaurant', 'vega', 'matter...   \n",
       "3   ['got', 'pasta', 'carbonara', 'wow', 'amazing'...   \n",
       "4   ['really', 'expecting', 'high', 'hope', 'went'...   \n",
       "5   ['food', 'good', 'service', 'lacking', 'loved'...   \n",
       "6   ['attentive', 'staff', 'even', 'though', 'busy...   \n",
       "7   ['great', 'restaurant', 'able', 'sit', 'outsid...   \n",
       "8   ['saw', 'killer', 'perform', 'sep', 'first', '...   \n",
       "9   ['hate', 'nothing', 'dislike', 'location', 'ki...   \n",
       "10  ['saw', 'place', 'popped', 'week', 'ago', 'int...   \n",
       "11  ['soso', 'banh', 'mi', 'sandwich', 'always', '...   \n",
       "12  ['fan', 'liberace', 'like', 'place', 'filled',...   \n",
       "13  ['nacho', 'daddy', 'literally', 'favorite', 'p...   \n",
       "14  ['food', 'good', 'much', 'traditional', 'typic...   \n",
       "15  ['best', 'ayce', 'sushi', 'restaurant', 'town'...   \n",
       "16  ['goto', 'every', 'lv', 'trip', 'simply', 'bes...   \n",
       "17  ['really', 'enjoy', 'coming', 'get', 'gelato',...   \n",
       "18  ['ate', 'breakfast', 'buffet', 'typical', 'buf...   \n",
       "19  ['love', 'love', 'love', 'food', 'amazing', 'k...   \n",
       "20  ['amazing', 'ambiance', 'soon', 'walked', 'sat...   \n",
       "21  ['place', 'joke', 'would', 'give', 'zero', 'st...   \n",
       "22  ['enchilada', 'nachosdelish', 'great', 'food',...   \n",
       "23  ['best', 'cannolis', 'ever', 'must', 'get', 'a...   \n",
       "24  ['stayed', 'hard', 'rock', 'daysand', 'loved',...   \n",
       "25  ['always', 'great', 'fast', 'great', 'every', ...   \n",
       "26  ['first', 'time', 'toronto', 'stumbled', 'upon...   \n",
       "27  ['excited', 'place', 'left', 'feeling', 'meh',...   \n",
       "28  ['rib', 'special', 'really', 'special', 'price...   \n",
       "29  ['came', 'wanting', 'vietnamese', 'coffee', 'd...   \n",
       "..                                                ...   \n",
       "70  ['tried', 'cabbage', 'roll', 'basically', 'mea...   \n",
       "71  ['food', 'good', 'dish', 'feed', 'people', 'di...   \n",
       "72  ['place', 'awesome', 'room', 'clean', 'spaciou...   \n",
       "73  ['kung', 'fu', 'tea', 'absolute', 'favorite', ...   \n",
       "74  ['bad', 'lot', 'repetitive', 'thing', 'expecti...   \n",
       "75  ['sat', 'front', 'row', 'ideal', 'perspective'...   \n",
       "76  ['seriously', 'wait', 'long', 'food', 'starvin...   \n",
       "77  ['husband', 'tried', 'every', 'hotdog', 'menu'...   \n",
       "78  ['amazing', 'war', 'museum', 'whats', 'free', ...   \n",
       "79  ['really', 'liked', 'place', 'club', 'lounge',...   \n",
       "80  ['great', 'place', 'relax', 'enjoy', 'gamble',...   \n",
       "81  ['show', 'freaking', 'amazing', 'first', 'walk...   \n",
       "82  ['place', 'amazing', 'wedding', 'february', 's...   \n",
       "83  ['best', 'vegan', 'restaurant', 'ever', 'far',...   \n",
       "84  ['tried', 'go', 'lunch', 'today', 'longer', 'h...   \n",
       "85  ['check', 'place', 'week', 'ago', 'taking', 'f...   \n",
       "86  ['one', 'fave', 'spot', 'downtown', 'food', 'a...   \n",
       "87  ['full', 'disclosure', 'im', 'target', 'audien...   \n",
       "88  ['definitely', 'recommend', 'place', 'like', '...   \n",
       "89  ['comedy', 'club', 'great', 'place', 'casual',...   \n",
       "90  ['food', 'amazingly', 'good', 'meat', 'tasty',...   \n",
       "91  ['vega', 'must', 'least', 'order', 'fat', 'tue...   \n",
       "92  ['spectacular', 'spectacular', 'spectacular', ...   \n",
       "93  ['going', 'ted', 'wiens', 'eastern', 'ave', 'p...   \n",
       "94  ['experience', 'brought', 'family', 'town', 'g...   \n",
       "95  ['youre', 'thinking', 'staying', 'convenience'...   \n",
       "96  ['great', 'hotel', 'great', 'service', 'classy...   \n",
       "97  ['first', 'visit', 'absolutely', 'loved', 'pla...   \n",
       "98  ['golden', 'knight', 'nacho', 'amazing', 'huge...   \n",
       "99  ['wow', 'european', 'wax', 'center', 'consiste...   \n",
       "\n",
       "                                       joined_comment  max_polarity  \\\n",
       "0   wish known pregnant seems make sense interesti...         1.000   \n",
       "1   wer mehrere tage la vega ist sollte nicht vers...         0.600   \n",
       "2   best thai restaurant vega matter best thai res...         0.613   \n",
       "3   got pasta carbonara wow amazing amazing amazin...         0.650   \n",
       "4   really expecting high hope went dessert ordere...         0.700   \n",
       "5   food good service lacking loved artichoke neve...         1.000   \n",
       "6   attentive staff even though busy thursday afte...         1.000   \n",
       "7   great restaurant able sit outside beautiful fa...         0.700   \n",
       "8   saw killer perform sep first time venue expect...         1.000   \n",
       "9   hate nothing dislike location kind far home ca...         1.000   \n",
       "10  saw place popped week ago intrigued looked lik...         1.000   \n",
       "11  soso banh mi sandwich always get original aka ...         1.000   \n",
       "12  fan liberace like place filled many interestin...         0.890   \n",
       "13  nacho daddy literally favorite place eat taco ...         1.000   \n",
       "14  food good much traditional typical chinese tak...         1.000   \n",
       "15  best ayce sushi restaurant town food orgasmic ...         1.000   \n",
       "16  goto every lv trip simply best ive ever outsid...         1.000   \n",
       "17  really enjoy coming get gelato favorite part p...         1.000   \n",
       "18  ate breakfast buffet typical buffet food wasnt...         0.875   \n",
       "19  love love love food amazing kind regular place...         1.000   \n",
       "20  amazing ambiance soon walked sat le minuet gre...         1.000   \n",
       "21  place joke would give zero star part went enti...         0.500   \n",
       "22  enchilada nachosdelish great food awesome serv...         1.000   \n",
       "23  best cannolis ever must get awesome love creme...         1.000   \n",
       "24  stayed hard rock daysand loved iti say expensi...         0.138   \n",
       "25  always great fast great every time go even dis...         1.000   \n",
       "26  first time toronto stumbled upon place yelp fo...         1.000   \n",
       "27  excited place left feeling meh primarily due s...         1.000   \n",
       "28  rib special really special price difference th...         1.000   \n",
       "29  came wanting vietnamese coffee dont make told ...         1.000   \n",
       "..                                                ...           ...   \n",
       "70  tried cabbage roll basically meat wrapped cabb...         1.000   \n",
       "71  food good dish feed people dim sum great appet...         1.000   \n",
       "72  place awesome room clean spacious lucky enough...         1.000   \n",
       "73  kung fu tea absolute favorite boba tea place l...         1.000   \n",
       "74  bad lot repetitive thing expecting like one la...         1.000   \n",
       "75  sat front row ideal perspective still beautifu...         1.000   \n",
       "76  seriously wait long food starving come ordered...         1.000   \n",
       "77  husband tried every hotdog menu cant pick didn...         1.000   \n",
       "78  amazing war museum whats free get donate like ...         1.000   \n",
       "79  really liked place club lounge unreal set top ...         1.000   \n",
       "80  great place relax enjoy gamble much staying wy...         0.721   \n",
       "81  show freaking amazing first walk instantly int...         1.000   \n",
       "82  place amazing wedding february service sale st...         1.000   \n",
       "83  best vegan restaurant ever far first place go ...         0.550   \n",
       "84  tried go lunch today longer herelocation vacan...         1.000   \n",
       "85  check place week ago taking family trip la veg...         1.000   \n",
       "86  one fave spot downtown food atmosphere always ...         0.655   \n",
       "87  full disclosure im target audience probally ho...         0.356   \n",
       "88  definitely recommend place like thai food atmo...         0.800   \n",
       "89  comedy club great place casual friend night ta...         1.000   \n",
       "90  food amazingly good meat tasty tortilla delici...         1.000   \n",
       "91  vega must least order fat tuesday cannot think...         1.000   \n",
       "92  spectacular spectacular spectacular wonderful ...         1.000   \n",
       "93  going ted wiens eastern ave past always speak ...         1.000   \n",
       "94  experience brought family town grandpa use eat...         1.000   \n",
       "95  youre thinking staying convenience many restau...         0.781   \n",
       "96  great hotel great service classy place emphasi...         1.000   \n",
       "97  first visit absolutely loved place staff nice ...         1.000   \n",
       "98  golden knight nacho amazing huge portion husba...         1.000   \n",
       "99  wow european wax center consistent excellent w...         1.000   \n",
       "\n",
       "    average_polarity  min_polarity  max_subjectivity ...  yup  zen  zero  \\\n",
       "0              0.148         -1.00             1.000 ...    0    0     1   \n",
       "1              0.013         -0.70             1.000 ...    0    0     0   \n",
       "2              0.112         -0.25             0.667 ...    0    0     0   \n",
       "3              0.427          0.20             1.000 ...    0    0     0   \n",
       "4              0.179          0.00             1.000 ...    0    0     0   \n",
       "5              0.297         -0.60             1.000 ...    0    1     0   \n",
       "6              0.258         -0.45             1.000 ...    0    0     0   \n",
       "7              0.189         -0.38             0.839 ...    0    0     0   \n",
       "8              0.139         -1.00             1.000 ...    0    0     0   \n",
       "9              0.016         -0.80             1.000 ...    0    0     0   \n",
       "10             0.136         -1.00             1.000 ...    0    0     2   \n",
       "11             0.179         -0.74             1.000 ...    1    0     0   \n",
       "12             0.159         -0.70             1.000 ...    0    0     0   \n",
       "13             0.348          0.00             1.000 ...    0    0     0   \n",
       "14             0.268         -0.50             1.000 ...    0    0     0   \n",
       "15             0.409         -0.17             1.000 ...    0    0     0   \n",
       "16             0.072         -1.00             1.000 ...    0    0     0   \n",
       "17             0.313         -0.70             1.000 ...    0    0     0   \n",
       "18             0.325         -0.25             0.917 ...    0    0     0   \n",
       "19             0.154         -1.00             1.000 ...    0    0     0   \n",
       "20             0.196         -1.00             1.000 ...    0    0     0   \n",
       "21             0.071         -0.32             0.800 ...    1    0     1   \n",
       "22             0.304          0.00             0.875 ...    0    0     0   \n",
       "23             0.289         -1.00             1.000 ...    0    0     0   \n",
       "24             0.138          0.14             0.417 ...    0    0     0   \n",
       "25             0.138         -0.75             1.000 ...    0    0     0   \n",
       "26             0.528          0.15             0.950 ...    0    0     0   \n",
       "27             0.216         -0.21             1.000 ...    0    0     0   \n",
       "28             0.201         -0.40             0.900 ...    0    0     0   \n",
       "29             0.355          0.00             1.000 ...    0    0     0   \n",
       "..               ...           ...               ... ...  ...  ...   ...   \n",
       "70             0.185         -0.70             1.000 ...    0    0     0   \n",
       "71             0.206         -0.41             1.000 ...    0    0     0   \n",
       "72             0.131         -0.49             1.000 ...    0    0     0   \n",
       "73             0.316         -0.60             1.000 ...    0    0     0   \n",
       "74             0.183         -1.00             1.000 ...    0    0     1   \n",
       "75             0.156         -1.00             1.000 ...    0    0     0   \n",
       "76             0.125         -0.75             1.000 ...    0    0     0   \n",
       "77             0.304         -1.00             1.000 ...    0    0     0   \n",
       "78             0.337         -0.19             0.950 ...    0    0     0   \n",
       "79             0.147         -1.00             1.000 ...    0    0     0   \n",
       "80             0.481          0.00             0.750 ...    0    0     0   \n",
       "81             0.200         -0.80             1.000 ...    0    0     0   \n",
       "82             0.174         -0.94             1.000 ...    0    0     0   \n",
       "83             0.400          0.25             0.650 ...    0    0     0   \n",
       "84             0.126         -0.75             1.000 ...    0    1     0   \n",
       "85             0.161         -0.40             1.000 ...    0    0     0   \n",
       "86             0.303          0.00             0.825 ...    0    0     0   \n",
       "87             0.148          0.00             0.833 ...    0    0     0   \n",
       "88             0.021         -1.00             1.000 ...    0    0     0   \n",
       "89             0.304         -1.00             1.000 ...    0    0     0   \n",
       "90             0.733          0.50             1.000 ...    0    0     0   \n",
       "91             0.307         -0.45             1.000 ...    0    0     0   \n",
       "92             0.343          0.00             0.925 ...    0    0     0   \n",
       "93             0.221         -0.75             1.000 ...    0    0     0   \n",
       "94             0.205         -1.00             1.000 ...    0    0     0   \n",
       "95             0.041         -0.40             1.000 ...    0    0     0   \n",
       "96             0.485          0.00             1.000 ...    0    0     0   \n",
       "97             0.360         -0.75             1.000 ...    0    0     0   \n",
       "98             0.199         -0.98             1.000 ...    0    0     0   \n",
       "99             0.195         -0.81             1.000 ...    0    0     0   \n",
       "\n",
       "    zodiac  zombie  zone  EI  FT  NS  PJ  \n",
       "0        0       1     0   0   1   1   1  \n",
       "1        0       0     0   0   1   0   1  \n",
       "2        0       0     0   0   0   0   1  \n",
       "3        0       0     0   0   0   0   1  \n",
       "4        0       0     0   0   0   1   1  \n",
       "5        0       0     0   1   0   0   1  \n",
       "6        0       0     0   0   0   1   1  \n",
       "7        0       0     0   0   0   1   1  \n",
       "8        0       0     0   1   1   0   0  \n",
       "9        0       0     0   1   1   1   0  \n",
       "10       0       1     1   0   0   1   0  \n",
       "11       0       0     0   0   0   0   1  \n",
       "12       0       0     0   0   0   0   1  \n",
       "13       0       0     0   0   0   1   1  \n",
       "14       0       0     0   0   0   0   1  \n",
       "15       0       0     0   0   0   1   1  \n",
       "16       0       0     0   0   0   1   1  \n",
       "17       0       0     0   0   0   1   1  \n",
       "18       0       0     0   0   0   1   1  \n",
       "19       0       0     0   0   1   0   1  \n",
       "20       0       0     0   0   0   1   1  \n",
       "21       0       0     0   0   0   0   1  \n",
       "22       0       0     0   0   0   1   1  \n",
       "23       0       0     0   0   0   0   1  \n",
       "24       0       0     0   0   0   0   1  \n",
       "25       0       0     0   0   0   0   1  \n",
       "26       0       0     0   0   0   0   1  \n",
       "27       0       0     0   0   0   0   1  \n",
       "28       0       0     0   0   0   0   1  \n",
       "29       0       0     0   0   0   0   1  \n",
       "..     ...     ...   ...  ..  ..  ..  ..  \n",
       "70       0       0     0   0   0   1   0  \n",
       "71       0       0     0   0   0   0   1  \n",
       "72       0       0     0   0   0   0   1  \n",
       "73       0       0     0   0   0   0   1  \n",
       "74       0       0     0   0   0   1   1  \n",
       "75       0       0     0   0   0   1   1  \n",
       "76       0       0     0   0   0   1   1  \n",
       "77       0       0     0   0   0   0   1  \n",
       "78       0       0     0   0   0   1   1  \n",
       "79       0       0     0   0   0   0   1  \n",
       "80       0       0     0   0   0   0   1  \n",
       "81       0       0     1   0   1   0   1  \n",
       "82       0       0     0   1   0   0   1  \n",
       "83       0       0     0   1   0   1   1  \n",
       "84       0       0     0   1   0   0   0  \n",
       "85       0       0     0   0   0   0   1  \n",
       "86       0       0     0   0   0   0   1  \n",
       "87       0       0     0   0   0   0   1  \n",
       "88       0       0     0   0   0   1   1  \n",
       "89       0       0     0   0   0   1   1  \n",
       "90       0       0     0   0   0   0   1  \n",
       "91       0       0     0   0   0   0   1  \n",
       "92       0       0     0   0   0   0   1  \n",
       "93       0       0     0   0   0   1   1  \n",
       "94       0       0     1   1   0   1   0  \n",
       "95       0       0     0   0   0   0   1  \n",
       "96       0       0     0   0   0   0   1  \n",
       "97       0       0     0   0   0   0   1  \n",
       "98       0       0     0   0   0   1   1  \n",
       "99       0       0     0   0   0   1   0  \n",
       "\n",
       "[100 rows x 5027 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_data_n.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data_n.to_csv('predicted_personality_types.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "E:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "predict_data_scaler = preprocessing.StandardScaler().fit(predict_data_r)\n",
    "predict_data_scaled = predict_data_scaler.transform(predict_data_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "E:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.20.1 when using version 0.20.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "#EI\n",
    "filename = 'logreg_EI.sav'\n",
    "loaded_model_EI = joblib.load(filename)\n",
    "result_EI = loaded_model_EI.predict(predict_data_scaled)\n",
    "#FT\n",
    "filename = 'logreg_FT.sav'\n",
    "loaded_model_FT = joblib.load(filename)\n",
    "result_FT = loaded_model_FT.predict(predict_data_scaled)\n",
    "#NS\n",
    "filename = 'randomforest_NS.sav'\n",
    "loaded_model_NS = joblib.load(filename)\n",
    "result_NS = loaded_model_NS.predict(predict_data_scaled)\n",
    "#PJ\n",
    "filename = 'logreg_PJ.sav'\n",
    "loaded_model_PJ = joblib.load(filename)\n",
    "result_PJ = loaded_model_PJ.predict(predict_data_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1' '1' '0' ... '1' '1' '1']\n",
      "['1' '0' '0' ... '1' '1' '1']\n",
      "['1' '0' '1' ... '0' '0' '0']\n",
      "['0' '1' '1' ... '0' '0' '0']\n"
     ]
    }
   ],
   "source": [
    "print(result_EI)\n",
    "print(result_FT)\n",
    "print(result_NS)\n",
    "print(result_PJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4921593020934126\n",
      "0.05596768495906864\n",
      "0.24661721113824508\n",
      "0.8742723740189529\n"
     ]
    }
   ],
   "source": [
    "print(sum(np.asfarray(result_EI,float)) / len(result_EI))\n",
    "print(sum(np.asfarray(result_FT,float)) / len(result_FT))\n",
    "print(sum(np.asfarray(result_NS,float)) / len(result_NS))\n",
    "print(sum(np.asfarray(result_PJ,float)) / len(result_PJ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_data_n = predict_data\n",
    "predict_data_n['EI'] = result_EI\n",
    "predict_data_n['FT'] = result_FT\n",
    "predict_data_n['NS'] = result_NS\n",
    "predict_data_n['PJ'] = result_PJ\n",
    "predict_data_n.to_csv('predicted_personality_types.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
